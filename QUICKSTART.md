# Gu√≠a de Inicio R√°pido - Local LLM Chat v2.0

Instrucciones para poner en marcha Local LLM Chat en pocos minutos.

## üÜï Novedades v2.0

- ‚úÖ **M√∫ltiples backends**: GGUF (llama.cpp) + Transformers (Hugging Face)
- ‚úÖ **Python 3.13**: Core funciona perfectamente
- ‚úÖ **RAG**: Disponible en Python 3.11/3.12
- ‚úÖ **Intercambiabilidad**: Cambia entre modelos GGUF y Transformers sin cambiar c√≥digo

## Requisitos

- **Python 3.8 - 3.13** (Core + GGUF + Transformers)
- **Python 3.11 - 3.12** (Para usar RAG)
- 4GB RAM m√≠nimo (8GB+ recomendado)

## Instalaci√≥n B√°sica (GGUF)

```bash
# 1. Clonar o navegar al proyecto
cd local-llm-chat

# 2. Crear entorno virtual
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate

# 3. Instalar solo GGUF (funciona en Python 3.13)
pip install -e .

# 4. ‚úÖ Listo para modelos GGUF!
```

## Instalaci√≥n Completa (GGUF + Transformers)

```bash
# Despu√©s de los pasos 1-2 anteriores:

# 3. Instalar con backend Transformers (incluye transformers + accelerate)
pip install -e ".[transformers]"

# O con TODO (Transformers + cuantizaci√≥n + accelerate + bitsandbytes)
pip install -e ".[quantization]"

# 4. ‚úÖ Listo para GGUF y Transformers!
```

**¬øQu√© incluye cada instalaci√≥n?**
- `[transformers]`: transformers + accelerate (gesti√≥n de memoria y balanceo de dispositivos)
- `[quantization]`: transformers + accelerate + bitsandbytes (cuantizaci√≥n 8-bit/4-bit)

**Nota sobre `accelerate`**: Se instala autom√°ticamente con `[transformers]`. Es necesario para usar `device_map="auto"` y gesti√≥n eficiente de memoria. Sin √©l, los modelos Transformers funcionan pero con selecci√≥n manual de dispositivo.

## Instalaci√≥n con RAG (Python 3.11/3.12 solamente)

‚ö†Ô∏è **Importante**: RAG requiere Python 3.11 o 3.12 (no funciona en 3.13)

```bash
# Verificar versi√≥n
python --version  # Debe ser 3.11.x o 3.12.x

# Instalar dependencias RAG
pip install -r requirements-rag.txt

# ‚úÖ Listo con RAG!
```

### Para GPU (CUDA) - Windows/Linux

```bash
# Instalar PyTorch con CUDA primero
pip uninstall torch torchvision torchaudio
pip install torch --index-url https://download.pytorch.org/whl/cu121

# Verificar CUDA
python verify_cuda.py
```

## Primera Ejecuci√≥n

### Opci√≥n A: Modelo GGUF Local

```bash
# Ejecutar la aplicaci√≥n
python main.py

# Seleccionar modelo GGUF de la lista
# Comenzar a chatear
```

### Opci√≥n B: Modelo Transformers (Hugging Face)

```bash
# Ejecutar con modelo de HuggingFace
python main.py --backend transformers --model "bigscience/bloom-560m"

# O especificar en la CLI durante ejecuci√≥n
python main.py
# Luego: /changemodel --backend transformers --model "microsoft/DialoGPT-small"
```

### Opci√≥n C: Descargar un modelo recomendado

```bash
# Ejecutar la aplicaci√≥n
python main.py

# La aplicaci√≥n mostrar√° recomendaciones basadas en tu RAM (GGUF y Transformers)
# Opci√≥n 1: Seleccionar un n√∫mero para descargar
# Opci√≥n 2: Descargar directamente por ID de HuggingFace
> /download meta-llama/Llama-3.1-8B-Instruct-GGUF
> /download bigscience/bloom-560m

# Esperar la descarga (puede tomar varios minutos)
# Comenzar a chatear
```

## Uso B√°sico

```
> Hola

[LLAMA-3] Hola, ¬øc√≥mo puedo ayudarte hoy?

> /help              # Mostrar todos los comandos
> /stats             # Mostrar estad√≠sticas de la sesi√≥n
> /history           # Mostrar historial de conversaci√≥n
> /exit              # Guardar y salir
```

## Comandos Comunes

### System Prompts

```bash
/preset coding      # Cargar preset de asistente de programaci√≥n
/preset creative    # Cargar preset de escritura creativa
/system Eres un experto en Python    # Prompt personalizado
/showsystem         # Ver prompt actual
```

### Gesti√≥n de Modelos

```bash
/models                           # Listar modelos locales y recomendaciones (GGUF y Transformers)
/download 1                       # Descargar modelo recomendado por n√∫mero
/download meta-llama/Llama-3.1-8B-Instruct-GGUF  # Descargar por ID de HuggingFace
/download bigscience/bloom-560m   # Descargar modelo Transformers directamente
/changemodel models/model.gguf    # Cambiar a GGUF local

# Cambiar a Transformers
/changemodel --backend transformers --model "meta-llama/Llama-2-7b-chat-hf"
/changemodel --backend transformers --model "bigscience/bloom-560m"
```

### Gesti√≥n de Sesi√≥n

```bash
/save               # Guardar conversaci√≥n ahora
/clear              # Limpiar historial
/stats              # Mostrar estad√≠sticas
/exit               # Guardar y salir
```

## Uso como Biblioteca

### Ejemplo 1: Modelo GGUF Local

```python
from local_llm_chat import UniversalChatClient, Config

# Cargar configuraci√≥n
config = Config()

# Backend GGUF (por defecto)
client = UniversalChatClient(
    backend="gguf",  # o simplemente omitir (es el default)
    model_path="models/llama-3.1-8b-instruct.gguf",
    system_prompt="Eres un asistente √∫til.",
    n_ctx=config.model.n_ctx,
    n_gpu_layers=config.model.n_gpu_layers,
    verbose=config.model.verbose,
    llm_config=config.llm
)

# Generar respuesta
response = client.infer("¬øQu√© es Python?")
print(response)

# Guardar sesi√≥n
client.save_log()
```

### Ejemplo 2: Modelo Transformers (Hugging Face)

```python
from local_llm_chat import UniversalChatClient

# Backend Transformers
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="microsoft/DialoGPT-small",  # o "model_path"
    system_prompt="Eres un asistente experto en Python.",
    device="cuda"  # o "cpu" o "mps" (Mac)
)

# Generar respuesta
response = client.infer("Explica las list comprehensions")
print(response)
```

### Ejemplo 3: Con Cuantizaci√≥n (8-bit)

```python
from local_llm_chat import UniversalChatClient

# Transformers con cuantizaci√≥n 8-bit
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="meta-llama/Llama-2-7b-chat-hf",
    load_in_8bit=True,  # Requiere bitsandbytes
    device="cuda"
)

response = client.infer("Hola, ¬øc√≥mo est√°s?")
print(response)
```

### Ejemplo 4: Cambio Din√°mico de Backend

```python
from local_llm_chat import UniversalChatClient

# Empezar con GGUF
client = UniversalChatClient(
    backend="gguf",
    model_path="models/llama-3.gguf"
)

# ... usar el modelo ...

# Cambiar a Transformers
client.change_model(
    model_path="bigscience/bloom-560m",  # o usar model_name_or_path en kwargs
    backend="transformers"
)

# Alternativa: usar model_name_or_path en kwargs
client.change_model(
    backend="transformers",
    model_name_or_path="bigscience/bloom-560m"
)

# Ahora usa Transformers
response = client.infer("Nueva pregunta")
print(response)
```

## Documentaci√≥n Adicional

- **Documentaci√≥n Completa**: Ver `README.md`
- **Configuraci√≥n**: Ver `CONFIG.md`
- **Arquitectura Multi-Backend**: Ver `doc/03.11.25/BACKENDS_ARCHITECTURE.md`
- **Ejemplos Completos**: Ver `EXAMPLES.md` (19 ejemplos)
- **Alias de Par√°metros**: Ver `doc/03.11.25/PARAMETER_ALIASES.md`
- **Fix Python 3.13**: Ver `doc/03.11.25/PYTHON_3.13_FIX.md`
- **Estructura del Proyecto**: Ver `PROJECT_STRUCTURE.md`
- **Verificar Instalaci√≥n**: Ejecutar `python verify_installation.py`

## Soluci√≥n de Problemas

### Python 3.13 - Error con RAG

```
ImportError: cannot import name 'Sequence' from 'collections'
```

**Soluci√≥n**: RAG requiere Python 3.11 o 3.12. Ver `doc/03.11.25/PYTHON_3.13_FIX.md`

```bash
# Opci√≥n 1: Usar sin RAG (Python 3.13)
pip install -r requirements.txt  # Solo core

# Opci√≥n 2: Cambiar a Python 3.11/3.12
pyenv install 3.12.0
pyenv local 3.12.0
pip install -r requirements-rag.txt
```

### El modelo GGUF no carga

```bash
# Verificar si el archivo existe
ls models/

# Intentar con un modelo diferente
# Ver README.md para recomendaciones
```

### Errores con Transformers

```bash
# Aseg√∫rate de tener instalado el backend
pip install -e ".[transformers]"

# Para modelos grandes, usa cuantizaci√≥n
pip install -e ".[quantization]"
```

### Errores de importaci√≥n

```bash
# Reinstalar en modo de desarrollo
pip install -e .

# Verificar instalaci√≥n
python verify_installation.py
```

### Sin memoria suficiente

```bash
# Para GGUF: Descargar cuantizaci√≥n menor (Q4 vs Q8)
/models  # Ver recomendaciones

# Para Transformers: Usar cuantizaci√≥n 8-bit o descargar modelo m√°s peque√±o
/models  # Ver recomendaciones seg√∫n tu RAM
/download bigscience/bloom-560m  # Modelo peque√±o para pruebas

# O usar cuantizaci√≥n 8-bit
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="modelo",
    load_in_8bit=True  # Reduce uso de memoria ~50%
)
```

## Funcionalidades v2.0

Ahora puedes:

- ‚úÖ Chatear con **modelos GGUF** (llama.cpp)
- ‚úÖ Chatear con **modelos Transformers** (Hugging Face)
- ‚úÖ **Cambiar entre backends** sin reiniciar
- ‚úÖ Usar **system prompts adaptativos** (presets o personalizados)
- ‚úÖ **RAG** para procesamiento de documentos (Python 3.11/3.12)
- ‚úÖ **Cuantizaci√≥n 8-bit/4-bit** para ahorrar memoria
- ‚úÖ **Aceleraci√≥n GPU** (CUDA/Metal)
- ‚úÖ Guardar y revisar conversaciones
- ‚úÖ Usar como **biblioteca de Python**
- ‚úÖ Compatible con **Python 3.8 - 3.13**
