[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "local-llm-chat"
version = "2.0.3"
description = "Local LLM Chat - Universal interface for local language models supporting GGUF and Transformers"
readme = "README.md"
authors = [
    {name = "Edu DÃ­az", email = "your.email@example.com"}
]
license = {text = "MIT"}
requires-python = ">=3.8"
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
keywords = ["llm", "local-llm", "chat", "gguf", "transformers", "llama-cpp", "ai", "chatbot", "offline"]

dependencies = [
    "llama-cpp-python>=0.3.0",
    "huggingface-hub>=0.20.0",
    "torch>=2.0.0",
    "psutil>=5.9.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "flake8>=6.0.0",
    "mypy>=1.0.0",
]
transformers = [
    "transformers>=4.35.0",
    "accelerate>=0.20.0",
]
quantization = [
    "transformers>=4.35.0",
    "accelerate>=0.20.0",
    "bitsandbytes>=0.41.0",
]
rag = [
    "chromadb>=0.5.0",
    "sentence-transformers>=2.2.0",
    "pypdf>=6.0.0",
]
rag-full = [
    "chromadb>=0.5.0",
    "sentence-transformers>=2.2.0",
    "pypdf>=6.0.0",
    "future>=1.0.0",
    "configparser>=5.0.0",
    "raganything>=1.2.0",
    "magic-pdf[full]>=0.8.0",
    "httpx>=0.28.0",
    "nest-asyncio>=1.5.0",
]
all = [
    "transformers>=4.35.0",
    "accelerate>=0.20.0",
    "bitsandbytes>=0.41.0",
    "chromadb>=0.5.0",
    "sentence-transformers>=2.2.0",
    "pypdf>=6.0.0",
    "future>=1.0.0",
    "configparser>=5.0.0",
    "raganything>=1.2.0",
    "magic-pdf[full]>=0.8.0",
    "httpx>=0.28.0",
    "nest-asyncio>=1.5.0",
]
# Note: CUDA-specific PyTorch installations should be done manually:
# pip install torch --index-url https://download.pytorch.org/whl/cu121
# or use the cuda extras which will be handled separately

[project.urls]
Homepage = "https://github.com/RGiskard7/local-llm-chat"
Repository = "https://github.com/RGiskard7/local-llm-chat"
Documentation = "https://github.com/RGiskard7/local-llm-chat#readme"
Issues = "https://github.com/RGiskard7/local-llm-chat/issues"

[project.scripts]
local-llm-chat = "local_llm_chat.cli:run_cli"
llm-chat = "local_llm_chat.cli:run_cli"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]
include = ["local_llm_chat*"]

[tool.black]
line-length = 100
target-version = ['py38', 'py39', 'py310', 'py311', 'py312']
include = '\.pyi?$'

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --strict-markers"

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false
