# ğŸ“‹ Changelog - Local LLM Chat

Registro completo de cambios y mejoras del proyecto.

---

## ğŸ“… 2025-11-03 â€” Fix Compatibilidad Python 3.13 + ActualizaciÃ³n Docs v2.0.1

**Archivos modificados:**
- `requirements.txt`
- `requirements-rag.txt` (nuevo)
- `README.md`
- `QUICKSTART.md` (actualizado a v2.0)
- `changelog.md`

### ğŸ”§ **Fix: Incompatibilidad RAG con Python 3.13**

**Problema identificado**:
```
ImportError: cannot import name 'Sequence' from 'collections'
```

La cadena de dependencias `raganything â†’ lightrag-hku â†’ future<1.0` es incompatible con Python 3.13, ya que el paquete `future` antiguo intenta importar `Sequence` desde `collections` en lugar de `collections.abc`.

**SoluciÃ³n implementada**:

1. **Dependencias RAG separadas**:
   - Comentadas en `requirements.txt` principal
   - Creado `requirements-rag.txt` especÃ­fico
   - Core del proyecto (GGUF + Transformers) funciona en Python 3.13

2. **DocumentaciÃ³n clara**:
   - Advertencia en `README.md` sobre versiones Python
   - Instrucciones especÃ­ficas para instalar RAG
   - Badge actualizado indicando compatibilidad

3. **Ruta de migraciÃ³n**:
   - Python 3.13: Core + GGUF + Transformers âœ…
   - Python 3.11/3.12: Todo incluyendo RAG âœ…
   - RAG disponible cuando `lightrag-hku` se actualice

**InstalaciÃ³n RAG ahora**:
```bash
# Solo si tienes Python 3.11 o 3.12
pip install -r requirements-rag.txt
```

**Beneficios**:
- âœ… No bloquea usuarios de Python 3.13
- âœ… Core del proyecto completamente funcional
- âœ… RAG disponible en versiones anteriores
- âœ… DocumentaciÃ³n clara de limitaciones

### ğŸ“š **ActualizaciÃ³n: QUICKSTART.md a v2.0**

**Problema**: `QUICKSTART.md` estaba desactualizado (v1.x), no reflejaba los cambios de v2.0.

**Cambios implementados**:

1. **SecciÃ³n de requisitos actualizada**:
   - Python 3.8-3.13 (core)
   - Python 3.11-3.12 (RAG)
   - Advertencias claras sobre limitaciones

2. **InstalaciÃ³n por niveles**:
   - BÃ¡sica (solo GGUF)
   - Completa (GGUF + Transformers)
   - Con RAG (Python 3.11/3.12)

3. **Ejemplos actualizados**:
   - âœ… Uso con backend GGUF
   - âœ… Uso con backend Transformers
   - âœ… CuantizaciÃ³n 8-bit
   - âœ… Cambio dinÃ¡mico de backends

4. **SoluciÃ³n de problemas ampliada**:
   - Fix Python 3.13
   - Errores Transformers
   - Problemas de memoria
   - GuÃ­a de cuantizaciÃ³n

5. **Comandos CLI actualizados**:
   - `/changemodel` con soporte multi-backend
   - Ejemplos con modelos HuggingFace
   - GestiÃ³n de backends

6. **Referencias actualizadas**:
   - Links a nueva documentaciÃ³n v2.0
   - `EXAMPLES.md`, `MIGRATION_v2.md`, `BACKENDS_ARCHITECTURE.md`
   - Fix Python 3.13

**Resultado**:
`QUICKSTART.md` ahora es una guÃ­a completa y actualizada para v2.0, con ejemplos prÃ¡cticos de ambos backends y soluciones a problemas comunes.

---

## ğŸ“… 2025-11-02 â€” Alias de ParÃ¡metros v2.0.1

**Archivos modificados:**
- `src/local_llm_chat/client.py`
- `README.md`
- `doc/PARAMETER_ALIASES.md` (nuevo)

### ğŸ“ **Mejora de Usabilidad: Alias model_path â†” model_name_or_path**

**Problema identificado**:
- Backend GGUF usaba `model_path`
- Backend Transformers usaba `model_name_or_path`
- Esto requerÃ­a recordar dos nombres diferentes segÃºn el backend

**SoluciÃ³n implementada**:
Ambos parÃ¡metros ahora son **completamente intercambiables** con cualquier backend:

```python
# GGUF - Ambas formas funcionan
client = UniversalChatClient(backend="gguf", model_path="models/llama.gguf")
client = UniversalChatClient(backend="gguf", model_name_or_path="models/llama.gguf")

# Transformers - Ambas formas funcionan
client = UniversalChatClient(backend="transformers", model_name_or_path="bigscience/bloom")
client = UniversalChatClient(backend="transformers", model_path="bigscience/bloom")
```

**CaracterÃ­sticas**:
- âœ… ValidaciÃ³n: Error claro si intentas usar ambos a la vez
- âœ… DocumentaciÃ³n: GuÃ­a completa en `doc/PARAMETER_ALIASES.md`
- âœ… Flexibilidad: Usa el nombre que prefieras
- âœ… ConvenciÃ³n: Respeta convenciones de ambas librerÃ­as
- âœ… Compatibilidad: CÃ³digo existente funciona sin cambios

**Recomendaciones** (pero ambos son vÃ¡lidos):
- GGUF â†’ `model_path` (mÃ¡s especÃ­fico para archivos locales)
- Transformers â†’ `model_name_or_path` (mÃ¡s descriptivo para nombres HF)

**Beneficios**:
- Mayor flexibilidad sin confusiÃ³n
- CÃ³digo mÃ¡s intuitivo segÃºn contexto
- Consistencia con convenciones originales de cada librerÃ­a
- Sin breaking changes

---

## ğŸ“… 2025-11-02 â€” Sistema Multi-Backend v2.0.0 ğŸ‰

### ğŸš€ **NUEVA CARACTERÃSTICA MAYOR: Sistema Multi-Backend**

**Archivos creados:**
- `src/local_llm_chat/backends/__init__.py`
- `src/local_llm_chat/backends/base.py`
- `src/local_llm_chat/backends/gguf_backend.py`
- `src/local_llm_chat/backends/transformers_backend.py`
- `doc/BACKENDS_ARCHITECTURE.md`

**Archivos modificados:**
- `src/local_llm_chat/client.py` (refactorizaciÃ³n completa)
- `src/local_llm_chat/__init__.py`
- `src/local_llm_chat/model_config.py`
- `README.md`
- `requirements.txt`
- `pyproject.toml`

### ğŸ“ **Cambios Implementados**

#### 1. **Arquitectura Modular de Backends**

**Nueva jerarquÃ­a**:
```
ModelBackend (Abstract Interface)
    â”œâ”€> GGUFBackend (llama-cpp-python)
    â””â”€> TransformersBackend (Hugging Face)
```

**Interfaz comÃºn** (`base.py`):
```python
class ModelBackend(ABC):
    def load_model() -> bool
    def generate(messages, max_tokens, ...) -> dict
    def unload_model()
    def get_model_info() -> dict
    def format_messages(messages, system_prompt) -> list
    @property is_loaded -> bool
```

**Ventajas**:
- âœ… Intercambiabilidad total entre backends
- âœ… FÃ¡cil agregar nuevos backends (vLLM, ONNX, etc.)
- âœ… Testing independiente por backend
- âœ… Sistema de prompts universal

#### 2. **GGUFBackend - Backend Original Refactorizado**

**Archivo**: `gguf_backend.py`

MigraciÃ³n de toda la lÃ³gica GGUF desde `UniversalChatClient` al backend dedicado:
- Carga de modelos .gguf locales
- DetecciÃ³n automÃ¡tica de tipo de modelo
- GPU automÃ¡tica (CUDA/Metal)
- System prompts adaptativos

**Compatibilidad**: 100% compatible con cÃ³digo existente

#### 3. **TransformersBackend - NUEVO**

**Archivo**: `transformers_backend.py`

Backend completamente nuevo para modelos Hugging Face:
- âœ… Modelos remotos desde HuggingFace Hub
- âœ… Modelos locales (PyTorch/SafeTensors)
- âœ… Multi-arquitectura (GPT, Llama, Mistral, BERT, Bloom, Falcon, etc.)
- âœ… CuantizaciÃ³n 8-bit/4-bit (bitsandbytes)
- âœ… Chat templates automÃ¡ticos
- âœ… GPU automÃ¡tica con accelerate
- âœ… System prompts adaptativos

**Ejemplos**:
```python
# Modelo remoto
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="bigscience/bloom-560m"
)

# Modelo local
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="/path/to/model",
    device="cuda"
)

# Con cuantizaciÃ³n
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    load_in_8bit=True
)
```

#### 4. **UniversalChatClient Refactorizado**

**Cambios mayores**:
- Ahora es un orquestador de backends (no contiene lÃ³gica de inferencia)
- Constructor con parÃ¡metro `backend` ("gguf" o "transformers")
- MÃ©todo `change_model()` soporta cambio de backend
- Interfaz pÃºblica sin cambios (compatibilidad hacia atrÃ¡s)

**Ejemplo de cambio dinÃ¡mico**:
```python
# Iniciar con GGUF
client = UniversalChatClient(
    backend="gguf",
    model_path="models/llama-3.2-3b.gguf"
)

# Cambiar a Transformers
client.change_model(
    backend="transformers",
    model_name_or_path="bigscience/bloom-560m"
)
```

#### 5. **DetecciÃ³n AutomÃ¡tica de Backend**

**Nuevas funciones en `model_config.py`**:
```python
detect_backend_type(model_identifier: str) -> str
is_gguf_model(model_identifier: str) -> bool
is_transformers_model(model_identifier: str) -> bool
```

**LÃ³gica de detecciÃ³n**:
- Si termina en `.gguf` â†’ "gguf"
- Si contiene `/` (nombre HF) â†’ "transformers"
- Si es directorio con `config.json` â†’ "transformers"
- Default â†’ "gguf" (compatibilidad)

#### 6. **Sistema de Dependencias Modular**

**Dependencias opcionales** (`pyproject.toml`):
```toml
[project.optional-dependencies]
transformers = ["transformers>=4.35.0", "accelerate>=0.20.0"]
quantization = ["transformers>=4.35.0", "accelerate>=0.20.0", "bitsandbytes>=0.41.0"]
rag = ["chromadb>=0.5.0", "sentence-transformers>=2.2.0", "pypdf>=3.0.0"]
all = [...]  # Todo incluido
```

**InstalaciÃ³n modular**:
```bash
pip install -e .                        # Solo GGUF
pip install -e ".[transformers]"        # + Transformers
pip install -e ".[quantization]"        # + cuantizaciÃ³n
pip install -e ".[all]"                 # Todo
```

#### 7. **Compatibilidad con RAG**

**Ambos backends funcionan con RAG** sin cambios:
```python
# Funciona con GGUF
client = UniversalChatClient(backend="gguf", ...)
rag = RAGManager(client, backend="simple")

# Funciona con Transformers
client = UniversalChatClient(backend="transformers", ...)
rag = RAGManager(client, backend="simple")
```

#### 8. **DocumentaciÃ³n Completa**

**Nuevo archivo**: `doc/BACKENDS_ARCHITECTURE.md`
- ExplicaciÃ³n detallada de la arquitectura
- Ejemplos de uso para ambos backends
- ComparaciÃ³n GGUF vs Transformers
- GuÃ­a de instalaciÃ³n
- Troubleshooting

**README actualizado**:
- SecciÃ³n "Backends Soportados"
- Ejemplos de uso para ambos backends
- Tabla comparativa
- Instrucciones de instalaciÃ³n modular

### ğŸ’¡ **Beneficios de la RefactorizaciÃ³n**

| Aspecto | Antes (v1.x) | Ahora (v2.0) |
|---------|--------------|--------------|
| **Backends** | Solo GGUF | GGUF + Transformers |
| **Arquitectura** | MonolÃ­tico | Modular |
| **Cambio de modelo** | Solo GGUF | Entre backends |
| **Extensibilidad** | DifÃ­cil | FÃ¡cil (interfaz comÃºn) |
| **Testing** | Acoplado | Independiente |
| **Modelos disponibles** | ~200 GGUF | Miles (HF + GGUF) |

### ğŸ¯ **Casos de Uso Nuevos**

1. **ExperimentaciÃ³n rÃ¡pida**:
   ```python
   # Probar modelo HF sin descargar GGUF
   client = UniversalChatClient(
       backend="transformers",
       model_name_or_path="bigscience/bloom-560m"
   )
   ```

2. **Fine-tuning local**:
   ```python
   # Usar modelo custom entrenado
   client = UniversalChatClient(
       backend="transformers",
       model_name_or_path="/path/to/finetuned/model"
   )
   ```

3. **ComparaciÃ³n de backends**:
   ```python
   # Comparar velocidad GGUF vs Transformers
   client.change_model(backend="gguf", ...)
   # vs
   client.change_model(backend="transformers", ...)
   ```

### ğŸ“Š **MÃ©tricas de ImplementaciÃ³n**

- **Archivos nuevos**: 5
- **Archivos modificados**: 6
- **LÃ­neas de cÃ³digo**: ~1500 nuevas
- **Tests**: Backend interface validada
- **DocumentaciÃ³n**: 2 documentos nuevos
- **Compatibilidad hacia atrÃ¡s**: 100%

### ğŸ”® **PrÃ³ximos Pasos**

Futuros backends posibles:
- vLLM Backend (inferencia ultra-rÃ¡pida)
- ONNX Backend (multiplataforma)
- TensorRT Backend (NVIDIA optimizado)
- OpenAI API Backend (compatibilidad con APIs)

---

## ğŸ“… 2025-10-25 â€” RAG Auto-Initialization on Startup

**Changed files:**
- `src/local_llm_chat/cli.py`

**Summary:**
Fixed RAG persistence bug. RAG now auto-initializes when there are documents from previous sessions, eliminating the need to manually `/load` already-loaded documents.

**Problem:**
- User loads document in Session 1
- Document persists to ChromaDB + metadata
- User opens Session 2
- `/rag on` fails with "use /load first" even though document is already loaded

**Solution:**
- Check for existing documents on startup
- Auto-initialize RAGManager if documents found
- Allow `/rag on` to initialize RAG if documents exist
- Show clear status messages

**New Behavior:**
```
Session 1: /load doc.pdf â†’ Document saved
Session 2: [Startup] â†’ "Found 1 document from previous session"
Session 2: /rag on â†’ "RAG mode activated" (works immediately)
```

**Benefits:**
- True document persistence across sessions
- No need to re-load existing documents
- Intuitive RAG workflow
- Clear user feedback

---

## ğŸ“… 2025-10-25 â€” Centralized Configuration System

**Changed files:**
- `src/local_llm_chat/config.py` (new)
- `src/local_llm_chat/config.json` (new)
- `src/local_llm_chat/rag/simple.py`
- `src/local_llm_chat/rag/raganything_backend.py`
- `src/local_llm_chat/rag/manager.py`
- `src/local_llm_chat/cli.py`
- `.gitignore`

**Summary:**
Implemented professional centralized configuration system using dataclasses + JSON. All RAG and LLM parameters are now configurable via code, JSON files, or environment variables.

**ğŸ¯ Configuration System:**

1. **Config Module** (`config.py`)
   - `RAGConfig`: chunk_size, chunk_overlap, top_k, max_context_tokens
   - `LLMConfig`: max_tokens, temperature, top_p, repeat_penalty
   - `Config`: Main class with hybrid loading strategy

2. **Loading Priority** (highest to lowest)
   - Constructor parameters (for library usage)
   - Environment variables (for deployment)
   - JSON file (for persistent config)
   - Default values (hardcoded)

3. **Default Configuration** (`config.json`)
   ```json
   {
     "rag": {
       "chunk_size": 150,
       "top_k": 1,
       "max_context_tokens": 800
     },
     "llm": {
       "max_tokens": 256,
       "temperature": 0.1
     }
   }
   ```

**Benefits:**
- âœ… **Centralized**: One place for all config
- âœ… **Flexible**: Code, JSON, or env vars
- âœ… **Library-friendly**: Constructor parameters
- âœ… **Deployment-ready**: Environment variables
- âœ… **Optimized**: Fast defaults for 3B models on CPU
- âœ… **Professional**: Standard industry pattern

---

## ğŸ“… 2025-10-24 â€” Document Persistence: RAG Sessions Survive Restarts

**Changed files:**
- `src/local_llm_chat/rag/simple.py`
- `src/local_llm_chat/rag/raganything_backend.py`

**Summary:**
Implemented document persistence across sessions using dual strategy: metadata.json files + database reconstruction fallback. Documents now persist between application restarts.

**ğŸ”„ Persistence Strategy:**

1. **Metadata File** (`rag_metadata.json`)
   - Saves list of loaded documents
   - Last updated timestamp
   - Backend type and document count
   - Fast and reliable primary method

2. **Database Reconstruction** (Fallback)
   - Extracts document list from ChromaDB/Knowledge Graph
   - Automatic if metadata file is missing/corrupted
   - Ensures data is never lost

**Benefits:**
- âœ… **Zero data loss**: Documents persist across sessions
- âœ… **Automatic recovery**: Works even if metadata is lost
- âœ… **Fast startup**: Instant restoration from metadata
- âœ… **User-friendly**: No manual reload required
- âœ… **Robust**: Dual-strategy ensures reliability

---

[Entradas anteriores continÃºan igual...]

---

*Formato del changelog*:
- ğŸ“… Fecha
- ğŸ› Bugs corregidos
- âœ¨ Nuevas caracterÃ­sticas
- ğŸ”§ Mejoras
- ğŸ“ DocumentaciÃ³n
- ğŸ—‚ï¸ Archivos modificados
- ğŸ’¡ Contexto/razÃ³n
