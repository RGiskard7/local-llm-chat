# üìã Changelog - Local LLM Chat

Registro completo de cambios y mejoras del proyecto.

---

## üìÖ 2025-11-04 ‚Äî v2.0.3: Modelos Transformers en ./models/ por consistencia

**Archivos modificados:**
- `src/local_llm_chat/cli.py`

**Resumen:**
Los modelos Transformers ahora se descargan en `./models/` en lugar del cach√© de HuggingFace, manteniendo consistencia con los modelos GGUF.

**Cambio principal:**

**Antes:**
- GGUF: se descargaba en `./models/` ‚úì
- Transformers: se descargaba en `~/.cache/huggingface/` ‚úó

**Ahora:**
- GGUF: se descarga en `./models/` ‚úì
- Transformers: se descarga en `./models/` ‚úì

**Implementaci√≥n:**
1. Uso de `snapshot_download()` para descargar modelos Transformers completos
2. Descarga en `./models/{org_model}/` con nombre normalizado
3. Cliente carga desde ruta local (no desde repo_id)
4. No usa cach√© de HuggingFace ‚Üí sin duplicidades

**Beneficios:**
- ‚úÖ Consistencia: ambos backends usan `./models/`
- ‚úÖ Organizaci√≥n: todos los modelos en un solo lugar
- ‚úÖ Portabilidad: f√°cil copiar/mover el directorio `./models/`
- ‚úÖ Sin duplicidades: no se almacena en cach√© de HF
- ‚úÖ Transparente: el usuario solo ejecuta `/download <num|id>`

**UX:**
```bash
> /download 12
[INFO] Downloading Transformers model to ./models/...
[SUCCESS] Model downloaded to: ./models/Qwen_Qwen2-0.5B
[SUCCESS] Model loaded successfully!
```

---

## üìÖ 2025-11-04 ‚Äî v2.0.2: Bugfixes cr√≠ticos y refactoring de arquitectura

**Archivos modificados:**
- `src/local_llm_chat/backends/transformers_backend.py`
- `src/local_llm_chat/client.py`
- `src/local_llm_chat/__init__.py`
- `pyproject.toml`

**Resumen:**
Arreglados bugs cr√≠ticos identificados en revisi√≥n de c√≥digo, refactorizaci√≥n de responsabilidades y fix de dependencia opcional `accelerate`.

**Bugs cr√≠ticos corregidos:**

1. **Bug: `repeat_penalty` no soportado en TransformersBackend**
   - **Problema**: `TransformersBackend.generate()` no aceptaba `repeat_penalty`, pero `client.infer()` lo pasaba
   - **Impacto**: Par√°metro ignorado silenciosamente en modelos Transformers
   - **Fix**: A√±adido par√°metro `repeat_penalty` con mapeo a `repetition_penalty` de HuggingFace
   - **Archivos**: `src/local_llm_chat/backends/transformers_backend.py` (l√≠neas 249, 314)

2. **Bug: Memory leak en `change_model()`**
   - **Problema**: Al cambiar de modelo, el anterior no se descargaba de memoria
   - **Impacto**: Consumo acumulativo de RAM/VRAM en cambios frecuentes
   - **Fix**: Llamada expl√≠cita a `backend.unload_model()` antes de cargar nuevo modelo
   - **Archivos**: `src/local_llm_chat/client.py` (l√≠neas 614-620)

**Mejoras importantes:**

3. **Validaci√≥n de par√°metros en `infer()`**
   - A√±adidas validaciones para `prompt`, `max_tokens`, `temperature`, `top_p`, `repeat_penalty`, `top_k`
   - Mensajes de error descriptivos con valores inv√°lidos
   - Previene errores en backends por par√°metros fuera de rango
   - **Archivos**: `src/local_llm_chat/client.py` (l√≠neas 403-445)

4. **Normalizaci√≥n de `device_map` en TransformersBackend**
   - **Problema**: Mezcla de strings simples ("cuda", "mps") con `device_map="auto"` de HuggingFace
   - **Fix**: Usar `device_map="auto"` de HF para balanceo inteligente cuando `device="auto"`
   - **Fix adicional**: Fallback inteligente cuando `accelerate` no est√° instalado
   - Si `accelerate` disponible: usa `device_map="auto"` (√≥ptimo)
   - Si no disponible: selecciona dispositivo directamente (cuda/mps/cpu)
   - Mejora mensajes informativos de detecci√≥n de hardware
   - **Archivos**: `src/local_llm_chat/backends/transformers_backend.py` (l√≠neas 184-219)

**Refactoring de arquitectura:**

5. **Separaci√≥n de responsabilidades: `ConversationManager`**
   - Nueva clase `ConversationManager` para gesti√≥n de historial y m√©tricas
   - Responsabilidad √∫nica: tracking de conversaciones
   - `UniversalChatClient` delega gesti√≥n de historial a `ConversationManager`
   - Mantiene API p√∫blica 100% compatible (sin breaking changes)
   - Facilita testing y mantenimiento futuro
   - **Archivos**: `src/local_llm_chat/client.py` (l√≠neas 21-82, m√∫ltiples delegaciones)

**Beneficios:**
- ‚úÖ Consistencia entre backends: GGUF y Transformers ahora aceptan los mismos par√°metros
- ‚úÖ Sin memory leaks: modelos se descargan correctamente
- ‚úÖ Validaci√≥n robusta: errores detectados temprano con mensajes claros
- ‚úÖ Mejor uso de GPU: device_map="auto" aprovecha balanceo de HuggingFace
- ‚úÖ C√≥digo m√°s mantenible: responsabilidades claramente separadas
- ‚úÖ Sin breaking changes: API p√∫blica sin modificaciones

**Documentaci√≥n actualizada:**
- README.md: A√±adida nota sobre `accelerate` en instalaci√≥n Transformers
- README.md: Nueva secci√≥n de troubleshooting para error de `accelerate`
- QUICKSTART.md: Explicaci√≥n de qu√© incluye cada instalaci√≥n
- Clarificado que `accelerate` es opcional pero recomendado

**Testing:**
- Probados todos los backends: GGUF y Transformers
- Verificados par√°metros: repeat_penalty, top_k, temperature, etc.
- Comprobada limpieza de memoria en cambio de modelos
- Validadas todas las validaciones de par√°metros
- Verificado fallback sin `accelerate` funciona correctamente

---

## üìÖ 2025-11-04 ‚Äî Feature: Comando /download mejorado con soporte para IDs de HuggingFace

**Archivos modificados:**
- `src/local_llm_chat/cli.py`
- `src/local_llm_chat/model_config.py`
- `src/local_llm_chat/utils.py`

**Resumen:**
Extendido el comando `/download` para aceptar tanto n√∫meros (recomendaciones) como IDs directos de HuggingFace, con detecci√≥n autom√°tica del backend.

**Cambios realizados:**

1. **Comando `/download` ahora acepta dos formatos**:
   - N√∫meros (comportamiento existente): `/download 1`
   - IDs de HuggingFace (nuevo): `/download meta-llama/Llama-3.1-8B-GGUF`

2. **Detecci√≥n autom√°tica de backend mejorada**:
   - Arreglado `detect_backend_type()` para detectar "GGUF" sin punto
   - Ahora reconoce correctamente repos como `bartowski/Llama-3.2-3B-Instruct-GGUF`
   - Mantiene compatibilidad con paths locales `.gguf`

3. **Soporte para ambos backends**:
   - GGUF: descarga archivo `.gguf` del repo
   - Transformers: carga directamente (auto-download)

**Casos de uso:**
```bash
# Desde recomendaciones (existente)
/download 1

# Modelo GGUF espec√≠fico (nuevo)
/download bartowski/Llama-3.2-3B-Instruct-GGUF

# Modelo Transformers espec√≠fico (nuevo)
/download microsoft/phi-2
/download bigscience/bloom-560m
```

**Impacto:**
- ‚úÖ M√°s flexible: acceso a cualquier modelo de HuggingFace
- ‚úÖ Retrocompatible: n√∫meros siguen funcionando igual
- ‚úÖ Sin duplicaci√≥n de c√≥digo: reutiliza l√≥gica existente
- ‚úÖ UX mejorada: menos pasos para probar modelos espec√≠ficos

---

## üìÖ 2025-11-04 ‚Äî Refactor: Eliminado hardcoding subjetivo en recomendaciones

**Archivos modificados:**
- `src/local_llm_chat/model_config.py`

**Resumen:**
Eliminado hardcoding subjetivo en el sistema de recomendaciones para usar solo m√©tricas objetivas de la API de HuggingFace.

**Cambios realizados:**

1. **Eliminado `priority_orgs` (hardcoding subjetivo)**:
   - Antes: priorizaba manualmente organizaciones espec√≠ficas (bigscience, meta-llama, etc.)
   - Ahora: usa solo `downloads` (m√©trica objetiva de HuggingFace)
   - Resultado: recomendaciones basadas en popularidad real, no preferencias subjetivas

2. **Creada constante `FULL_PRECISION_SIZE_MULTIPLIER`**:
   - Antes: `estimated_size = base_gb * 2` (hardcoded)
   - Ahora: `estimated_size = base_gb * FULL_PRECISION_SIZE_MULTIPLIER`
   - Mejor mantenibilidad y claridad del c√≥digo

3. **Simplificado algoritmo de ordenamiento**:
   - Antes: `sort(key=lambda x: (not x['priority'], -x['downloads']))`
   - Ahora: `sort(key=lambda x: -x['downloads'])`
   - M√°s simple y transparente

**Impacto:**
- ‚úÖ Sin hardcoding subjetivo
- ‚úÖ Recomendaciones basadas en datos reales (downloads)
- ‚úÖ C√≥digo m√°s mantenible
- ‚úÖ Organizaciones nuevas/emergentes se incluyen autom√°ticamente

---

## üìÖ 2025-11-04 ‚Äî Feature: Sistema de recomendaciones para Transformers + detecci√≥n MPS

**Archivos modificados:**
- `src/local_llm_chat/model_config.py`
- `src/local_llm_chat/utils.py`
- `src/local_llm_chat/cli.py`
- `src/local_llm_chat/backends/transformers_backend.py`

**Resumen:**
Extendido el sistema de recomendaciones inteligentes para incluir modelos Transformers adem√°s de GGUF, con detecci√≥n autom√°tica de hardware (incluyendo Metal/MPS en macOS).

**Cambios realizados:**

1. **Nueva funci√≥n `get_transformers_recommendations()` en `model_config.py`**:
   - Consulta la API de HuggingFace para modelos populares de Transformers
   - Filtra bas√°ndose en hardware detectado (usa thresholds espec√≠ficos ya que Transformers necesita m√°s RAM)
   - Prioriza organizaciones conocidas (bigscience, meta-llama, mistralai, etc.)
   - Retorna top 10 modelos compatibles con el hardware del usuario

2. **Thresholds espec√≠ficos para Transformers**:
   - < 8GB RAM: modelos tiny (500M-560M par√°metros)
   - 8-16GB RAM: modelos small (1B-1.5B par√°metros)
   - 16-32GB RAM: modelos medium (3B-7B par√°metros)
   - > 32GB RAM: modelos large (7B-8B par√°metros)

3. **Actualizado `show_available_models()` en `utils.py`**:
   - Muestra dos secciones separadas: "GGUF MODELS" y "TRANSFORMERS MODELS"
   - Numeraci√≥n continua entre ambas secciones
   - Indica caracter√≠sticas de cada backend (GGUF = r√°pido en CPU, Transformers = m√°s modelos disponibles)
   - Retorna lista combinada para el comando `/download`

4. **CLI actualizada para manejar ambos backends**:
   - Detecta autom√°ticamente el tipo de backend de cada modelo recomendado
   - GGUF: descarga archivo `.gguf` expl√≠citamente (como antes)
   - Transformers: carga directamente usando el nombre del modelo (descarga autom√°tica por HuggingFace)
   - Muestra el backend en los mensajes de descarga/carga

5. **Detecci√≥n autom√°tica de Metal/MPS en TransformersBackend**:
   - Prioridad de detecci√≥n: CUDA > MPS > CPU
   - Detecta Apple Silicon (Metal Performance Shaders) autom√°ticamente
   - Usa `torch.backends.mps.is_available()` para verificar MPS
   - Selecciona dtype autom√°ticamente seg√∫n GPU disponible (float16 en GPU, float32 en CPU)
   - Mensajes informativos sobre qu√© GPU se detect√≥

**Ejemplo de uso:**
```bash
# CLI muestra ahora ambos tipos
$ python main.py
GGUF MODELS (Recommended - Fast on CPU)
  1. bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
  2. ...

TRANSFORMERS MODELS (More RAM, any HF model)
  6. bigscience/bloom-560m
  7. ...

# Descargar GGUF (√≠ndice 1-5)
> /download 1

# Cargar Transformers (√≠ndice 6+)
> /download 6
[INFO] Backend: TRANSFORMERS
[INFO] Loading Transformers model (auto-download from HuggingFace Hub)...
[TRANSFORMERS] Detected Metal (MPS) - Apple Silicon
```

**Impacto:**
- ‚úÖ Paridad de experiencia entre GGUF y Transformers
- ‚úÖ Usuarios no necesitan conocer nombres exactos de modelos
- ‚úÖ Transformers ahora soporta Apple Silicon autom√°ticamente
- ‚úÖ Thresholds ajustados seg√∫n requisitos reales de memoria
- ‚úÖ Mismo flujo de trabajo para ambos backends

---

## üìÖ 2025-11-04 ‚Äî Fix: Eliminadas dependencias CUDA inv√°lidas en pyproject.toml

**Archivos modificados:**
- `pyproject.toml`

**Resumen:**
Eliminadas las dependencias opcionales `cuda` y `cuda118` que inclu√≠an `--index-url`, formato inv√°lido seg√∫n PEP 508 que causaba errores de parseo.

**Problema identificado:**
- Las dependencias opcionales `cuda` y `cuda118` (l√≠neas 83-88) conten√≠an `--index-url https://download.pytorch.org/whl/cu121`
- PEP 508 no permite especificar URLs de √≠ndice directamente en especificaciones de dependencias
- Esto causaba errores de parseo al instalar el paquete

**Cambios realizados:**
1. Eliminadas las dependencias opcionales `cuda` y `cuda118`
2. A√±adido comentario explicativo sobre instalaci√≥n manual de PyTorch con CUDA
3. `torch>=2.0.0` en `dependencies` principal sigue instalando PyTorch CPU por defecto

**Nota:**
PyTorch con CUDA debe instalarse manualmente:
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu121
# o
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Impacto:**
- ‚úÖ `pyproject.toml` ahora es v√°lido seg√∫n PEP 508
- ‚úÖ Eliminados errores de parseo durante instalaci√≥n
- ‚úÖ Instalaci√≥n manual de PyTorch CUDA documentada claramente

---

## üìÖ 2025-11-04 ‚Äî Bugfix: top_k parameter en GGUFBackend

**Archivos modificados:**
- `src/local_llm_chat/backends/gguf_backend.py`

**Resumen:**
Corregido bug donde `GGUFBackend.generate()` no aceptaba el par√°metro `top_k`, causando inconsistencia con otros backends y p√©rdida silenciosa del par√°metro.

**Problema identificado:**
- `client.infer()` pasaba `top_k` expl√≠citamente a todos los backends (l√≠nea 437)
- `TransformersBackend.generate()` aceptaba `top_k: int = 50` correctamente
- `GGUFBackend.generate()` **no** ten√≠a `top_k` en su firma, solo `**kwargs`
- El par√°metro `top_k` se perd√≠a silenciosamente y no se pasaba a `llm.create_chat_completion()`

**Cambios realizados:**
1. A√±adido `top_k: int = 40` a la firma de `GGUFBackend.generate()`
2. Actualizado docstring para documentar el par√°metro
3. Pasado `top_k` a `llm.create_chat_completion()`

**Firma actualizada:**
```python
def generate(
    self, 
    messages: List[Dict[str, str]], 
    max_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repeat_penalty: float = 1.1,
    top_k: int = 40,  # ‚úÖ A√ëADIDO
    **kwargs
) -> Dict[str, Any]:
```

**Impacto:**
- ‚úÖ Consistencia entre backends (GGUF y Transformers)
- ‚úÖ El par√°metro `top_k` ahora se respeta correctamente
- ‚úÖ Mejor control sobre la generaci√≥n de texto
- ‚úÖ Interfaz unificada para todos los backends

---

## üìÖ 2025-11-03 ‚Äî Renombrado de simple.py a simple_rag_backend.py (Coherencia)

**Archivos modificados:**
- `src/local_llm_chat/rag/simple.py` ‚Üí `simple_rag_backend.py` (renombrado)
- `src/local_llm_chat/rag/__init__.py`
- `src/local_llm_chat/rag/manager.py`
- `README.md`
- `PROJECT_STRUCTURE.md`
- `CONFIG.md`

**Resumen:**
Renombrado `simple.py` a `simple_rag_backend.py` para mantener coherencia con la nomenclatura del proyecto. Todos los backends ahora siguen el mismo patr√≥n de nombres: `*_backend.py`.

**Motivaci√≥n:**
- **Coherencia interna**: `raganything_backend.py` ten√≠a sufijo, pero `simple.py` no
- **Coherencia con backends/**: `gguf_backend.py`, `transformers_backend.py` usan el mismo patr√≥n
- **Est√°ndar de la industria**: Django, Keras, Celery usan `*_backend.py` para implementaciones intercambiables
- **Claridad**: El nombre indica expl√≠citamente que es un backend RAG

**Cambios realizados:**
1. Renombrado f√≠sico del archivo
2. Actualizados imports en `rag/__init__.py` y `rag/manager.py`
3. Actualizada documentaci√≥n en README, PROJECT_STRUCTURE y CONFIG

**Arquitectura resultante:**
```
src/local_llm_chat/
‚îú‚îÄ‚îÄ backends/
‚îÇ   ‚îú‚îÄ‚îÄ gguf_backend.py          ‚úì Coherente
‚îÇ   ‚îî‚îÄ‚îÄ transformers_backend.py  ‚úì Coherente
‚îî‚îÄ‚îÄ rag/
    ‚îú‚îÄ‚îÄ simple_rag_backend.py    ‚úì Coherente (antes: simple.py)
    ‚îî‚îÄ‚îÄ raganything_backend.py   ‚úì Coherente
```

**Beneficios:**
- ‚úÖ Nomenclatura consistente en todo el proyecto
- ‚úÖ Sigue est√°ndares de la industria (Strategy/Backend pattern)
- ‚úÖ M√°s f√°cil de entender para nuevos desarrolladores
- ‚úÖ Documentaci√≥n actualizada

---

## üìÖ 2025-11-03 ‚Äî Fix Imports Condicionales RAG + Mejora requirements-rag.txt

**Archivos modificados:**
- `src/local_llm_chat/rag/__init__.py`
- `requirements-rag.txt`
- `pyproject.toml`

**Resumen:**
Implementados imports condicionales para los backends RAG, siguiendo el mismo patr√≥n profesional que `backends/__init__.py`. Esto previene errores de importaci√≥n cuando las dependencias RAG opcionales no est√°n instaladas.

**Cambios realizados:**

1. **Imports condicionales en `rag/__init__.py`**:
   - `SimpleRAG` y `RAGAnythingBackend` ahora se importan con try/except
   - Evita errores cuando chromadb, sentence-transformers o raganything no est√°n instalados
   - Mismo patr√≥n que el m√≥dulo `backends`

2. **Reorganizaci√≥n de `requirements-rag.txt`**:
   - Secciones claras: SimpleRAG (ligero) vs RAG-Anything (pesado)
   - Comentarios profesionales con instrucciones de instalaci√≥n
   - Facilita instalar solo SimpleRAG sin los conflictos de magic-pdf

3. **Actualizaci√≥n de `pyproject.toml`**:
   - `pypdf` actualizado de 3.0.0 a 6.0.0 (consistencia)
   - A√±adidos `future>=1.0.0` y `configparser>=5.0.0` a `rag-full`
   - Mejora la compatibilidad con Python 3.11/3.12

**Beneficios:**
- ‚úÖ El proyecto funciona sin errores aunque RAG no est√© instalado
- ‚úÖ Instalaci√≥n simple de SimpleRAG sin conflictos de dependencias
- ‚úÖ Documentaci√≥n clara sobre qu√© instalar seg√∫n las necesidades
- ‚úÖ Patr√≥n consistente con el resto del proyecto (backends)

**Instrucciones de instalaci√≥n:**
```bash
# Solo SimpleRAG (recomendado)
pip install chromadb sentence-transformers pypdf

# RAG completo (opcional, pesado)
pip install -r requirements-rag.txt
```

---

## üìÖ 2025-11-03 ‚Äî Resoluci√≥n de Conflictos de Merge

**Archivos modificados:**
- `QUICKSTART.md`
- `README.md`
- `requirements.txt`
- `src/local_llm_chat/__init__.py`
- `src/local_llm_chat/client.py`

**Resumen:**
Resueltos todos los conflictos de merge entre las ramas `develop` y `main`. Se mantuvo la versi√≥n 2.0 del proyecto con soporte completo para m√∫ltiples backends (GGUF + Transformers), preservando todas las funcionalidades avanzadas y la documentaci√≥n actualizada.

**Archivos resueltos:**
- ‚úÖ QUICKSTART.md - Mantenida versi√≥n v2.0 con documentaci√≥n multi-backend
- ‚úÖ README.md - Preservada documentaci√≥n completa v2.0
- ‚úÖ requirements.txt - Mantenidas dependencias con Transformers opcionales
- ‚úÖ src/local_llm_chat/__init__.py - Preservadas exportaciones de backends
- ‚úÖ src/local_llm_chat/client.py - Mantenida implementaci√≥n multi-backend

---

## üìÖ 2025-11-03 ‚Äî Fix Compatibilidad Python 3.13 + Actualizaci√≥n Docs v2.0.1

**Archivos modificados:**
- `requirements.txt`
- `requirements-rag.txt` (nuevo)
- `README.md`
- `QUICKSTART.md` (actualizado a v2.0)
- `changelog.md`

### üîß **Fix: Incompatibilidad RAG con Python 3.13**

**Problema identificado**:
```
ImportError: cannot import name 'Sequence' from 'collections'
```

La cadena de dependencias `raganything ‚Üí lightrag-hku ‚Üí future<1.0` es incompatible con Python 3.13, ya que el paquete `future` antiguo intenta importar `Sequence` desde `collections` en lugar de `collections.abc`.

**Soluci√≥n implementada**:

1. **Dependencias RAG separadas**:
   - Comentadas en `requirements.txt` principal
   - Creado `requirements-rag.txt` espec√≠fico
   - Core del proyecto (GGUF + Transformers) funciona en Python 3.13

2. **Documentaci√≥n clara**:
   - Advertencia en `README.md` sobre versiones Python
   - Instrucciones espec√≠ficas para instalar RAG
   - Badge actualizado indicando compatibilidad

3. **Ruta de migraci√≥n**:
   - Python 3.13: Core + GGUF + Transformers ‚úÖ
   - Python 3.11/3.12: Todo incluyendo RAG ‚úÖ
   - RAG disponible cuando `lightrag-hku` se actualice

**Instalaci√≥n RAG ahora**:
```bash
# Solo si tienes Python 3.11 o 3.12
pip install -r requirements-rag.txt
```

**Beneficios**:
- ‚úÖ No bloquea usuarios de Python 3.13
- ‚úÖ Core del proyecto completamente funcional
- ‚úÖ RAG disponible en versiones anteriores
- ‚úÖ Documentaci√≥n clara de limitaciones

### üìö **Actualizaci√≥n: QUICKSTART.md a v2.0**

**Problema**: `QUICKSTART.md` estaba desactualizado (v1.x), no reflejaba los cambios de v2.0.

**Cambios implementados**:

1. **Secci√≥n de requisitos actualizada**:
   - Python 3.8-3.13 (core)
   - Python 3.11-3.12 (RAG)
   - Advertencias claras sobre limitaciones

2. **Instalaci√≥n por niveles**:
   - B√°sica (solo GGUF)
   - Completa (GGUF + Transformers)
   - Con RAG (Python 3.11/3.12)

3. **Ejemplos actualizados**:
   - ‚úÖ Uso con backend GGUF
   - ‚úÖ Uso con backend Transformers
   - ‚úÖ Cuantizaci√≥n 8-bit
   - ‚úÖ Cambio din√°mico de backends

4. **Soluci√≥n de problemas ampliada**:
   - Fix Python 3.13
   - Errores Transformers
   - Problemas de memoria
   - Gu√≠a de cuantizaci√≥n

5. **Comandos CLI actualizados**:
   - `/changemodel` con soporte multi-backend
   - Ejemplos con modelos HuggingFace
   - Gesti√≥n de backends

6. **Referencias actualizadas**:
   - Links a nueva documentaci√≥n v2.0
   - `EXAMPLES.md`, `MIGRATION_v2.md`, `BACKENDS_ARCHITECTURE.md`
   - Fix Python 3.13

**Resultado**:
`QUICKSTART.md` ahora es una gu√≠a completa y actualizada para v2.0, con ejemplos pr√°cticos de ambos backends y soluciones a problemas comunes.

---

## üìÖ 2025-11-02 ‚Äî Alias de Par√°metros v2.0.1

**Archivos modificados:**
- `src/local_llm_chat/client.py`
- `README.md`
- `doc/PARAMETER_ALIASES.md` (nuevo)

### üìù **Mejora de Usabilidad: Alias model_path ‚Üî model_name_or_path**

**Problema identificado**:
- Backend GGUF usaba `model_path`
- Backend Transformers usaba `model_name_or_path`
- Esto requer√≠a recordar dos nombres diferentes seg√∫n el backend

**Soluci√≥n implementada**:
Ambos par√°metros ahora son **completamente intercambiables** con cualquier backend:

```python
# GGUF - Ambas formas funcionan
client = UniversalChatClient(backend="gguf", model_path="models/llama.gguf")
client = UniversalChatClient(backend="gguf", model_name_or_path="models/llama.gguf")

# Transformers - Ambas formas funcionan
client = UniversalChatClient(backend="transformers", model_name_or_path="bigscience/bloom")
client = UniversalChatClient(backend="transformers", model_path="bigscience/bloom")
```

**Caracter√≠sticas**:
- ‚úÖ Validaci√≥n: Error claro si intentas usar ambos a la vez
- ‚úÖ Documentaci√≥n: Gu√≠a completa en `doc/PARAMETER_ALIASES.md`
- ‚úÖ Flexibilidad: Usa el nombre que prefieras
- ‚úÖ Convenci√≥n: Respeta convenciones de ambas librer√≠as
- ‚úÖ Compatibilidad: C√≥digo existente funciona sin cambios

**Recomendaciones** (pero ambos son v√°lidos):
- GGUF ‚Üí `model_path` (m√°s espec√≠fico para archivos locales)
- Transformers ‚Üí `model_name_or_path` (m√°s descriptivo para nombres HF)

**Beneficios**:
- Mayor flexibilidad sin confusi√≥n
- C√≥digo m√°s intuitivo seg√∫n contexto
- Consistencia con convenciones originales de cada librer√≠a
- Sin breaking changes

---

## üìÖ 2025-11-02 ‚Äî Sistema Multi-Backend v2.0.0 üéâ

### üöÄ **NUEVA CARACTER√çSTICA MAYOR: Sistema Multi-Backend**

**Archivos creados:**
- `src/local_llm_chat/backends/__init__.py`
- `src/local_llm_chat/backends/base.py`
- `src/local_llm_chat/backends/gguf_backend.py`
- `src/local_llm_chat/backends/transformers_backend.py`
- `doc/BACKENDS_ARCHITECTURE.md`

**Archivos modificados:**
- `src/local_llm_chat/client.py` (refactorizaci√≥n completa)
- `src/local_llm_chat/__init__.py`
- `src/local_llm_chat/model_config.py`
- `README.md`
- `requirements.txt`
- `pyproject.toml`

### üìù **Cambios Implementados**

#### 1. **Arquitectura Modular de Backends**

**Nueva jerarqu√≠a**:
```
ModelBackend (Abstract Interface)
    ‚îú‚îÄ> GGUFBackend (llama-cpp-python)
    ‚îî‚îÄ> TransformersBackend (Hugging Face)
```

**Interfaz com√∫n** (`base.py`):
```python
class ModelBackend(ABC):
    def load_model() -> bool
    def generate(messages, max_tokens, ...) -> dict
    def unload_model()
    def get_model_info() -> dict
    def format_messages(messages, system_prompt) -> list
    @property is_loaded -> bool
```

**Ventajas**:
- ‚úÖ Intercambiabilidad total entre backends
- ‚úÖ F√°cil agregar nuevos backends (vLLM, ONNX, etc.)
- ‚úÖ Testing independiente por backend
- ‚úÖ Sistema de prompts universal

#### 2. **GGUFBackend - Backend Original Refactorizado**

**Archivo**: `gguf_backend.py`

Migraci√≥n de toda la l√≥gica GGUF desde `UniversalChatClient` al backend dedicado:
- Carga de modelos .gguf locales
- Detecci√≥n autom√°tica de tipo de modelo
- GPU autom√°tica (CUDA/Metal)
- System prompts adaptativos

**Compatibilidad**: 100% compatible con c√≥digo existente

#### 3. **TransformersBackend - NUEVO**

**Archivo**: `transformers_backend.py`

Backend completamente nuevo para modelos Hugging Face:
- ‚úÖ Modelos remotos desde HuggingFace Hub
- ‚úÖ Modelos locales (PyTorch/SafeTensors)
- ‚úÖ Multi-arquitectura (GPT, Llama, Mistral, BERT, Bloom, Falcon, etc.)
- ‚úÖ Cuantizaci√≥n 8-bit/4-bit (bitsandbytes)
- ‚úÖ Chat templates autom√°ticos
- ‚úÖ GPU autom√°tica con accelerate
- ‚úÖ System prompts adaptativos

**Ejemplos**:
```python
# Modelo remoto
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="bigscience/bloom-560m"
)

# Modelo local
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="/path/to/model",
    device="cuda"
)

# Con cuantizaci√≥n
client = UniversalChatClient(
    backend="transformers",
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    load_in_8bit=True
)
```

#### 4. **UniversalChatClient Refactorizado**

**Cambios mayores**:
- Ahora es un orquestador de backends (no contiene l√≥gica de inferencia)
- Constructor con par√°metro `backend` ("gguf" o "transformers")
- M√©todo `change_model()` soporta cambio de backend
- Interfaz p√∫blica sin cambios (compatibilidad hacia atr√°s)

**Ejemplo de cambio din√°mico**:
```python
# Iniciar con GGUF
client = UniversalChatClient(
    backend="gguf",
    model_path="models/llama-3.2-3b.gguf"
)

# Cambiar a Transformers
client.change_model(
    backend="transformers",
    model_name_or_path="bigscience/bloom-560m"
)
```

#### 5. **Detecci√≥n Autom√°tica de Backend**

**Nuevas funciones en `model_config.py`**:
```python
detect_backend_type(model_identifier: str) -> str
is_gguf_model(model_identifier: str) -> bool
is_transformers_model(model_identifier: str) -> bool
```

**L√≥gica de detecci√≥n**:
- Si termina en `.gguf` ‚Üí "gguf"
- Si contiene `/` (nombre HF) ‚Üí "transformers"
- Si es directorio con `config.json` ‚Üí "transformers"
- Default ‚Üí "gguf" (compatibilidad)

#### 6. **Sistema de Dependencias Modular**

**Dependencias opcionales** (`pyproject.toml`):
```toml
[project.optional-dependencies]
transformers = ["transformers>=4.35.0", "accelerate>=0.20.0"]
quantization = ["transformers>=4.35.0", "accelerate>=0.20.0", "bitsandbytes>=0.41.0"]
rag = ["chromadb>=0.5.0", "sentence-transformers>=2.2.0", "pypdf>=3.0.0"]
all = [...]  # Todo incluido
```

**Instalaci√≥n modular**:
```bash
pip install -e .                        # Solo GGUF
pip install -e ".[transformers]"        # + Transformers
pip install -e ".[quantization]"        # + cuantizaci√≥n
pip install -e ".[all]"                 # Todo
```

#### 7. **Compatibilidad con RAG**

**Ambos backends funcionan con RAG** sin cambios:
```python
# Funciona con GGUF
client = UniversalChatClient(backend="gguf", ...)
rag = RAGManager(client, backend="simple")

# Funciona con Transformers
client = UniversalChatClient(backend="transformers", ...)
rag = RAGManager(client, backend="simple")
```

#### 8. **Documentaci√≥n Completa**

**Nuevo archivo**: `doc/BACKENDS_ARCHITECTURE.md`
- Explicaci√≥n detallada de la arquitectura
- Ejemplos de uso para ambos backends
- Comparaci√≥n GGUF vs Transformers
- Gu√≠a de instalaci√≥n
- Troubleshooting

**README actualizado**:
- Secci√≥n "Backends Soportados"
- Ejemplos de uso para ambos backends
- Tabla comparativa
- Instrucciones de instalaci√≥n modular

### üí° **Beneficios de la Refactorizaci√≥n**

| Aspecto | Antes (v1.x) | Ahora (v2.0) |
|---------|--------------|--------------|
| **Backends** | Solo GGUF | GGUF + Transformers |
| **Arquitectura** | Monol√≠tico | Modular |
| **Cambio de modelo** | Solo GGUF | Entre backends |
| **Extensibilidad** | Dif√≠cil | F√°cil (interfaz com√∫n) |
| **Testing** | Acoplado | Independiente |
| **Modelos disponibles** | ~200 GGUF | Miles (HF + GGUF) |

### üéØ **Casos de Uso Nuevos**

1. **Experimentaci√≥n r√°pida**:
   ```python
   # Probar modelo HF sin descargar GGUF
   client = UniversalChatClient(
       backend="transformers",
       model_name_or_path="bigscience/bloom-560m"
   )
   ```

2. **Fine-tuning local**:
   ```python
   # Usar modelo custom entrenado
   client = UniversalChatClient(
       backend="transformers",
       model_name_or_path="/path/to/finetuned/model"
   )
   ```

3. **Comparaci√≥n de backends**:
   ```python
   # Comparar velocidad GGUF vs Transformers
   client.change_model(backend="gguf", ...)
   # vs
   client.change_model(backend="transformers", ...)
   ```

### üìä **M√©tricas de Implementaci√≥n**

- **Archivos nuevos**: 5
- **Archivos modificados**: 6
- **L√≠neas de c√≥digo**: ~1500 nuevas
- **Tests**: Backend interface validada
- **Documentaci√≥n**: 2 documentos nuevos
- **Compatibilidad hacia atr√°s**: 100%

### üîÆ **Pr√≥ximos Pasos**

Futuros backends posibles:
- vLLM Backend (inferencia ultra-r√°pida)
- ONNX Backend (multiplataforma)
- TensorRT Backend (NVIDIA optimizado)
- OpenAI API Backend (compatibilidad con APIs)

---

## üìÖ 2025-10-25 ‚Äî RAG Auto-Initialization on Startup

**Changed files:**
- `src/local_llm_chat/cli.py`

**Summary:**
Fixed RAG persistence bug. RAG now auto-initializes when there are documents from previous sessions, eliminating the need to manually `/load` already-loaded documents.

**Problem:**
- User loads document in Session 1
- Document persists to ChromaDB + metadata
- User opens Session 2
- `/rag on` fails with "use /load first" even though document is already loaded

**Solution:**
- Check for existing documents on startup
- Auto-initialize RAGManager if documents found
- Allow `/rag on` to initialize RAG if documents exist
- Show clear status messages

**New Behavior:**
```
Session 1: /load doc.pdf ‚Üí Document saved
Session 2: [Startup] ‚Üí "Found 1 document from previous session"
Session 2: /rag on ‚Üí "RAG mode activated" (works immediately)
```

**Benefits:**
- True document persistence across sessions
- No need to re-load existing documents
- Intuitive RAG workflow
- Clear user feedback

---

## üìÖ 2025-10-25 ‚Äî Centralized Configuration System

**Changed files:**
- `src/local_llm_chat/config.py` (new)
- `src/local_llm_chat/config.json` (new)
- `src/local_llm_chat/rag/simple.py`
- `src/local_llm_chat/rag/raganything_backend.py`
- `src/local_llm_chat/rag/manager.py`
- `src/local_llm_chat/cli.py`
- `.gitignore`

**Summary:**
Implemented professional centralized configuration system using dataclasses + JSON. All RAG and LLM parameters are now configurable via code, JSON files, or environment variables.

**üéØ Configuration System:**

1. **Config Module** (`config.py`)
   - `RAGConfig`: chunk_size, chunk_overlap, top_k, max_context_tokens
   - `LLMConfig`: max_tokens, temperature, top_p, repeat_penalty
   - `Config`: Main class with hybrid loading strategy

2. **Loading Priority** (highest to lowest)
   - Constructor parameters (for library usage)
   - Environment variables (for deployment)
   - JSON file (for persistent config)
   - Default values (hardcoded)

3. **Default Configuration** (`config.json`)
   ```json
   {
     "rag": {
       "chunk_size": 150,
       "top_k": 1,
       "max_context_tokens": 800
     },
     "llm": {
       "max_tokens": 256,
       "temperature": 0.1
     }
   }
   ```

**Benefits:**
- ‚úÖ **Centralized**: One place for all config
- ‚úÖ **Flexible**: Code, JSON, or env vars
- ‚úÖ **Library-friendly**: Constructor parameters
- ‚úÖ **Deployment-ready**: Environment variables
- ‚úÖ **Optimized**: Fast defaults for 3B models on CPU
- ‚úÖ **Professional**: Standard industry pattern

---

## üìÖ 2025-10-24 ‚Äî Document Persistence: RAG Sessions Survive Restarts

**Changed files:**
- `src/local_llm_chat/rag/simple.py`
- `src/local_llm_chat/rag/raganything_backend.py`

**Summary:**
Implemented document persistence across sessions using dual strategy: metadata.json files + database reconstruction fallback. Documents now persist between application restarts.

**üîÑ Persistence Strategy:**

1. **Metadata File** (`rag_metadata.json`)
   - Saves list of loaded documents
   - Last updated timestamp
   - Backend type and document count
   - Fast and reliable primary method

2. **Database Reconstruction** (Fallback)
   - Extracts document list from ChromaDB/Knowledge Graph
   - Automatic if metadata file is missing/corrupted
   - Ensures data is never lost

**Benefits:**
- ‚úÖ **Zero data loss**: Documents persist across sessions
- ‚úÖ **Automatic recovery**: Works even if metadata is lost
- ‚úÖ **Fast startup**: Instant restoration from metadata
- ‚úÖ **User-friendly**: No manual reload required
- ‚úÖ **Robust**: Dual-strategy ensures reliability

---

[Entradas anteriores contin√∫an igual...]

---

*Formato del changelog*:
- üìÖ Fecha
- üêõ Bugs corregidos
- ‚ú® Nuevas caracter√≠sticas
- üîß Mejoras
- üìù Documentaci√≥n
- üóÇÔ∏è Archivos modificados
- üí° Contexto/raz√≥n
